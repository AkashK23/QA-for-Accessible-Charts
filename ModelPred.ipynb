{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "file_name = \"questionsData.txt\"\n",
    "with open(file_name) as data_file:\n",
    "        for l in data_file:\n",
    "            data.append(l.strip().split(\":\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tell me the median', 'type3'], ['what is the eighth y-axis label', 'type1'], ['tell me the last thirteen x-axis labels', 'type1'], ['which Tahnee is the maximum', 'type3'], ['read me the last six y-axis values', 'type1'], ['what is the Bryanna for proceedings', 'type2'], ['what is the y-axis title', 'type1'], ['what is the kangaroo when act is Brittaney', 'type2'], ['tell me the range value', 'type3'], ['what are the last nineteen x-axis values', 'type1']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(data)\n",
    "print(data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed = {}\n",
    "import numpy as np\n",
    "embedding_file = \"glove.6B.50d.txt\"\n",
    "count = 0\n",
    "with open(embedding_file, encoding=\"utf8\") as embeddings:\n",
    "    for line in embeddings:\n",
    "        count += 1\n",
    "        values = line.split()\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        word_embed[values[0]] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "unkStr = '-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011 0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802 0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912 0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227 0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107 -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544 -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303 -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408 0.00653003  0.01685157'\n",
    "unkList = unkStr.split()\n",
    "unkVec = np.asarray(unkList, \"float32\")\n",
    "word_embed['<unk>'] = unkVec\n",
    "word_embed['<pad>'] = np.zeros(50, dtype=\"float32\")\n",
    "print(word_embed['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tell', 'me', 'the', 'median', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['what', 'is', 'the', 'eighth', 'y-axis', 'label', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['tell', 'me', 'the', 'last', 'thirteen', 'x-axis', 'labels', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['which', 'Tahnee', 'is', 'the', 'maximum', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['read', 'me', 'the', 'last', 'six', 'y-axis', 'values', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['what', 'is', 'the', 'Bryanna', 'for', 'proceedings', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['what', 'is', 'the', 'y-axis', 'title', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['what', 'is', 'the', 'kangaroo', 'when', 'act', 'is', 'Brittaney', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['tell', 'me', 'the', 'range', 'value', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['what', 'are', 'the', 'last', 'nineteen', 'x-axis', 'values', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "['type3', 'type1', 'type1', 'type3', 'type1', 'type2', 'type1', 'type2', 'type3', 'type1']\n",
      "['read', 'me', 'the', 'y-axis', 'name', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "questions = [i[0].split() for i in data]\n",
    "types = [i[1] for i in data]\n",
    "for i in range(0, len(questions)):\n",
    "    for j in range(0, 15-len(questions[i])):\n",
    "        questions[i].append('<pad>')\n",
    "print(questions[0:10])\n",
    "print(types[0:10])\n",
    "print(questions[len(questions) -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "padData = np.zeros((len(data)*15, 50))\n",
    "labels = np.zeros(len(data))\n",
    "label_id = {'type1': 0, 'type2': 1, 'type3': 2}\n",
    "count = 0\n",
    "for i in range(0, len(questions)):\n",
    "    for j in range(0, len(questions[i])):\n",
    "        if questions[i][j] in word_embed:\n",
    "            padData[count] = word_embed[questions[i][j]]\n",
    "        else:\n",
    "            padData[count] = word_embed['<unk>']\n",
    "        count += 1\n",
    "    labels[i] = label_id[types[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315000, 50)\n",
      "[2. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(padData.shape)\n",
    "print(labels[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_utils\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import math\n",
    "\n",
    "\n",
    "class Type_Classifier(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, h_size=50, d=50, classes=3, num_layers=2):\n",
    "        super(Type_Classifier, self).__init__()\n",
    "        self.hidden_size = h_size\n",
    "        self.layers = num_layers\n",
    "        \n",
    "        # one lSTMs, one for first sentence, one for second\n",
    "        self.lstm = nn.LSTM(d, h_size, num_layers, batch_first=True)\n",
    "        \n",
    "        #after LSTM, output goes through 2 fully connected layers\n",
    "        self.fc1 = nn.Linear(h_size, 100)\n",
    "        self.fc2 = nn.Linear(100, classes)\n",
    "        \n",
    "    def display(self):\n",
    "        \n",
    "        for param in self.parameters():\n",
    "            print(param.data.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # takes in 1 input x. x is a 3x15x50 tensor. \n",
    "        # Each of the 15 rows is the word embedding for the respective word in the sentence\n",
    "        \n",
    "        # x goes through LSTM layer\n",
    "        h0 = torch.zeros(self.layers, x.size(0), self.hidden_size) \n",
    "        c0 = torch.zeros(self.layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0,c0))\n",
    "        \n",
    "        # concatenate output\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # send through a tanh layer then a linear layer\n",
    "        outF = self.fc1(out)\n",
    "        m = nn.Tanh()\n",
    "        outF = m(outF)\n",
    "        outF = self.fc2(outF)\n",
    "\n",
    "        return outF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 50])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "model = Type_Classifier(h_size=100, d=50, classes=3, num_layers=2)\n",
    "model.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_index_gen(batch_size, size):\n",
    "    batch_indexer = []\n",
    "    start = 0\n",
    "    while start < size:\n",
    "        end = start + batch_size\n",
    "        if end > size:\n",
    "            end = size\n",
    "        batch_indexer.append((start, end))\n",
    "        start = end\n",
    "    return batch_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [1/7000.0], Loss: 1.0854\n",
      "Epoch [1/3], Step [1001/7000.0], Loss: 0.0133\n",
      "Epoch [1/3], Step [2001/7000.0], Loss: 0.0010\n",
      "Epoch [1/3], Step [3001/7000.0], Loss: 0.0004\n",
      "Epoch [1/3], Step [4001/7000.0], Loss: 0.0002\n",
      "Epoch [1/3], Step [5001/7000.0], Loss: 0.0002\n",
      "Epoch [1/3], Step [6001/7000.0], Loss: 0.0001\n",
      "Epoch [2/3], Step [1/7000.0], Loss: 0.0008\n",
      "Epoch [2/3], Step [1001/7000.0], Loss: 0.0002\n",
      "Epoch [2/3], Step [2001/7000.0], Loss: 0.0003\n",
      "Epoch [2/3], Step [3001/7000.0], Loss: 0.0001\n",
      "Epoch [2/3], Step [4001/7000.0], Loss: 0.0000\n",
      "Epoch [2/3], Step [5001/7000.0], Loss: 0.0002\n",
      "Epoch [2/3], Step [6001/7000.0], Loss: 0.0001\n",
      "Epoch [3/3], Step [1/7000.0], Loss: 0.0001\n",
      "Epoch [3/3], Step [1001/7000.0], Loss: 0.0002\n",
      "Epoch [3/3], Step [2001/7000.0], Loss: 0.0001\n",
      "Epoch [3/3], Step [3001/7000.0], Loss: 0.0000\n",
      "Epoch [3/3], Step [4001/7000.0], Loss: 0.0001\n",
      "Epoch [3/3], Step [5001/7000.0], Loss: 0.0001\n",
      "Epoch [3/3], Step [6001/7000.0], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
    "for epoch in range(EPOCHS):\n",
    "    itr = 0\n",
    "    # using batch size of 3\n",
    "    for start, end in batch_index_gen(3, len(labels)):\n",
    "\n",
    "        inp = padData[start*15:end*15]\n",
    "        lab = labels[start:end]\n",
    "        \n",
    "        tensData = torch.FloatTensor(inp)\n",
    "        tensData = torch.reshape(tensData, [lab.shape[0], 15, 50])\n",
    "        \n",
    "        x = tensData\n",
    "        \n",
    "        # x is the inputs for the model\n",
    "        # again x is 3x15x50 sized tensors\n",
    "        # it is 3 because using batch size 3\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(x)\n",
    "        tensorLab = torch.LongTensor(lab.astype(int))\n",
    "        tensorLab = torch.reshape(tensorLab, [-1])\n",
    "#         print(tensorLab)\n",
    "        loss = criterion(outputs, tensorLab.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (itr) % 1000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{EPOCHS}], Step [{itr+1}/{len(labels)/3}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        itr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testD = []\n",
    "file_name = \"testData.txt\"\n",
    "with open(file_name) as data_file:\n",
    "        for l in data_file:\n",
    "            testD.append(l.strip().split(\":\"))\n",
    "questions = [i[0].split() for i in testD]\n",
    "types = [i[1] for i in testD]\n",
    "for i in range(0, len(questions)):\n",
    "    for j in range(0, 15-len(questions[i])):\n",
    "        questions[i].append('<pad>')\n",
    "padData = np.zeros((len(data)*15, 50))\n",
    "labels = np.zeros(len(data))\n",
    "label_id = {'type1': 0, 'type2': 1, 'type3': 2}\n",
    "count = 0\n",
    "for i in range(0, len(questions)):\n",
    "    for j in range(0, len(questions[i])):\n",
    "        if questions[i][j] in word_embed:\n",
    "            padData[count] = word_embed[questions[i][j]]\n",
    "        else:\n",
    "            padData[count] = word_embed['<unk>']\n",
    "        count += 1\n",
    "    labels[i] = label_id[types[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.9952380952381 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# print(quesData.shape)\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for start, end in batch_index_gen(3, len(labels)):\n",
    "        inp = padData[start*15:end*15]\n",
    "        lab = labels[start:end]\n",
    "        \n",
    "        tensData = torch.FloatTensor(inp)\n",
    "        tensData = torch.reshape(tensData, [lab.shape[0], 15, 50])\n",
    "        \n",
    "        x = tensData\n",
    "        \n",
    "        # x is the inputs for the model\n",
    "        # again x is 3x15x50 sized tensors\n",
    "        # it is 3 because using batch size 3\n",
    "        \n",
    "        outputs = model(x)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        tensorLab = torch.LongTensor(lab)\n",
    "        tensorLab = torch.reshape(tensorLab, [3])\n",
    "        \n",
    "        n_samples += tensorLab.size(0)\n",
    "        n_correct += (predicted == tensorLab).sum().item()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "# nltk.download()\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.13333333333334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "numWrong = 0\n",
    "dataSets = 100\n",
    "for i in range(0,dataSets):\n",
    "    dfName = \"data/dataframe\" + str(i) + \".csv\"\n",
    "    quesName = \"data/ques\" + str(i) + \".txt\"\n",
    "    df = pd.read_csv(dfName)\n",
    "    products_list = [df.columns.values.tolist()][0]\n",
    "    yLab = products_list[0]\n",
    "    xLab = products_list[1]\n",
    "    nums = df[yLab].values.tolist()\n",
    "    randLabs = df[xLab].values.tolist()\n",
    "    \n",
    "    testD = []\n",
    "    with open(quesName) as data_file:\n",
    "        for l in data_file:\n",
    "            testD.append(l.strip().split(\":\"))\n",
    "    ques = [i[0] for i in testD]\n",
    "    ans = [i[1].split(\",\") for i in testD]\n",
    "    \n",
    "    for j in range(0,15):\n",
    "        prediction = predict(ques[j])\n",
    "        qList = ques[j].split()\n",
    "        tags = nltk.pos_tag(qList)\n",
    "        output = []\n",
    "        if prediction == 0:\n",
    "            output = type_0(tags, randLabs, nums, xLab, yLab)\n",
    "        elif prediction == 1:\n",
    "            output = type_1(tags, randLabs, nums)\n",
    "        else:\n",
    "            output = type_2(tags, randLabs, nums)\n",
    "        if output != ans[j]:\n",
    "            numWrong += 1\n",
    "acc = (((dataSets*15)-numWrong)/(dataSets*15)) * 100\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(val):\n",
    "    valList = val.split()\n",
    "    quesData = np.zeros((15, 50), \"float32\")\n",
    "    for i in range(0, len(valList)):\n",
    "        if valList[i] in word_embed:\n",
    "            quesData[i] = word_embed[valList[i]]\n",
    "        else:\n",
    "            quesData[i] = word_embed['<unk>']\n",
    "    model.eval()\n",
    "#     print(quesData.shape)\n",
    "    with torch.no_grad():\n",
    "        tens = torch.FloatTensor(quesData)\n",
    "        tens = torch.reshape(tens, [1, 15, 50])\n",
    "        outputs = model(tens)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction = predicted[0].numpy()\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "import random\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if type 0\n",
    "wordToInt = {}\n",
    "for i in range(1,30):\n",
    "    wordToInt[p.number_to_words(p.ordinal(i))] = i\n",
    "    wordToInt[p.number_to_words(i)] = i\n",
    "def type_0(tagged, randLabs, nums, xLab, yLab):\n",
    "    \n",
    "    nns = []\n",
    "    jjs = []\n",
    "    position = 0\n",
    "    last = False\n",
    "    first = False\n",
    "    ordinal = False\n",
    "    single = False\n",
    "    xaxis = True\n",
    "    title = False\n",
    "\n",
    "    ind = 0\n",
    "    for pair in tagged:\n",
    "        if pair[1] == 'NN':\n",
    "            nns.append(pair[0])\n",
    "            if pair[0] == \"y-axis\":\n",
    "                xaxis = False\n",
    "            if pair[0] in wordToInt:\n",
    "                position = wordToInt[pair[0]]\n",
    "                if tagged[ind+1][0] == \"to\" or tagged[ind+1][0] == \"from\":\n",
    "                    single = True\n",
    "            if pair[0] == \"last\" or pair[0] == \"end\":\n",
    "                last = True\n",
    "        elif pair[1] == 'JJ' or pair[1] == 'CD':\n",
    "            jjs.append(pair[0])\n",
    "            if pair[0] == \"y-axis\":\n",
    "                xaxis = False\n",
    "            try:\n",
    "                position = int(pair[0])\n",
    "            except:\n",
    "                pass\n",
    "            if pair[0] in wordToInt:\n",
    "                position = wordToInt[pair[0]]\n",
    "                if tagged[ind+1][0] == \"to\" or tagged[ind+1][0] == \"from\":\n",
    "                    single = True\n",
    "                try:\n",
    "                    w2n.word_to_num(pair[0])\n",
    "                    ordinal = True\n",
    "                except:\n",
    "                    pass\n",
    "            if pair[0] == \"first\":\n",
    "                first = True\n",
    "            if pair[0] == \"last\" or pair[0] == \"end\":\n",
    "                last = True\n",
    "        ind +=1\n",
    "    finalp = []\n",
    "    if first: \n",
    "        if position == 0:\n",
    "            finalp.append(0)\n",
    "        elif single:\n",
    "            finalp.append(position-1)\n",
    "        else:\n",
    "            for i in range(1, position+1):\n",
    "                finalp.append(i-1)\n",
    "    elif last:\n",
    "        if position == 0:\n",
    "            finalp.append(-1)\n",
    "        elif single:\n",
    "            finalp.append(position*-1)\n",
    "        else:\n",
    "            for i in range(position*-1, 0):\n",
    "                finalp.append(i)\n",
    "    else:\n",
    "        if position == 0:\n",
    "            if xaxis:\n",
    "                return [xLab]\n",
    "            else:\n",
    "                return [yLab]\n",
    "        finalp.append(position-1)\n",
    "    retVals = []\n",
    "    if xaxis:\n",
    "        for i in range(0, len(finalp)):\n",
    "            retVals.append(randLabs[finalp[i]])\n",
    "    else:\n",
    "        for i in range(0, len(finalp)):\n",
    "            retVals.append(str(nums[finalp[i]]))\n",
    "    return retVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_1(tagged, randLabs, nums):\n",
    "    for pair in tagged:\n",
    "        if pair[0] in randLabs and (pair[1] == \"NN\" or pair[1] == \"JJ\" or pair[1] == \"NNP\"):\n",
    "            return [str(nums[randLabs.index(pair[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import statistics\n",
    "def type_2(tagged, randLabs, nums):\n",
    "    operations = [\"mean\", \"average\", \"median\", \"mode\", \"range\", \"max\", \"largest\", \"maximum\", \"biggest\", \"tallest\", \"highest\", \"min\", \"smallest\", \"minimum\", \"lowest\", \"shortest\"]\n",
    "    operation = \"\"\n",
    "    opN = \"\"\n",
    "    opJ = \"\"\n",
    "    firstN = True\n",
    "    for pair in tagged:\n",
    "        if pair[0] in operations:\n",
    "            operation = pair[0]\n",
    "        elif pair[1] == \"NN\" and firstN:\n",
    "            opN = pair[0]\n",
    "            firstN = False\n",
    "        elif pair[1] == \"JJ\":\n",
    "            opJ = pair[0]\n",
    "    if operation == \"\":\n",
    "        minDis = 1\n",
    "        for op in operations:\n",
    "            minVal = min(distance.cosine(word_embed[op],word_embed[opJ]), distance.cosine(word_embed[op],word_embed[opN]))\n",
    "            if minVal < minDis:\n",
    "                minDis = minVal\n",
    "                operation = op\n",
    "    cat = False\n",
    "    if tagged[0][0] == \"which\" or tagged[0][0] == \"Which\":\n",
    "        cat = True\n",
    "    opInt = operations.index(operation)\n",
    "    \n",
    "    finAns = ''\n",
    "    if cat:\n",
    "        if opInt == 2:\n",
    "            el = statistics.median_high(nums)\n",
    "            finAns = randLabs[nums.index(el)]\n",
    "        elif opInt < 11:\n",
    "            el = max(nums)\n",
    "            finAns = randLabs[nums.index(el)]\n",
    "        else:\n",
    "            el = min(nums)\n",
    "            finAns = randLabs[nums.index(el)]\n",
    "    else:\n",
    "        if opInt < 2:\n",
    "            finAns = statistics.mean(nums)\n",
    "        elif opInt < 3:\n",
    "            finAns = statistics.median(nums)\n",
    "        elif opInt < 4:\n",
    "            try:\n",
    "                finAns = statistics.mode(nums)\n",
    "            except:\n",
    "                finAns = \"none\"\n",
    "        elif opInt < 5:\n",
    "            finAns = max(nums) - min(nums)\n",
    "        elif opInt < 11:\n",
    "            finAns = max(nums)\n",
    "        else:\n",
    "            finAns = min(nums)\n",
    "    return [str(finAns)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
